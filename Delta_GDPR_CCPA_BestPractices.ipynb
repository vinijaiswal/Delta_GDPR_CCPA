{"cells":[{"cell_type":"markdown","source":["# ````Tech Talk````\n<img src=\"https://docs.delta.io/latest/_static/delta-lake-logo.png\" width=200/>\n###Addressing GDPR and CCPA Scenarios with Delta Lake and Apache Sparkâ„¢\n\nDelta Lake is a very effective tool for addressing GDPR and CCPA compliance requirements, because its structured data storage layer adds transactional capabilities to your data lake. \n\nIn this demonstration, we will be reviewing:\n\n- How to **convert your existing data to Delta**   \n- How to delete and clean up personal information quickly and efficiently with **Delta's DELETE capabilities**\n  - How to speed up DELETE FROM queries using **broadcasting**\n  - How to speed up DELETE FROM queries using Databricks **Z-Ordering**\n- **Audit History**\n- **Vaccum** and Retention\n- Recommendations on setting Efficient Pipelines\n  - **Pseudonymization** of the Personal Identifiers\n  - **Compaction** and **Optimize**\n  \nFor any errors/questions about the notebook: Please contact vini@databricks.com"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"30bad0a3-a36c-4cb6-ac7c-100e6832a795","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["d\n#### SETUP\n\n\nTo run this notebook, we have to [create a cluster](https://docs.databricks.com/clusters/create.html)\n- Cluster used for demo: **Databricks Runtime 6.6 ** | **3 worker nodes** and a **driver** each of type **xlarge, 30 GB memory and 4 cores**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f1e06c1d-4e6b-4fa2-a547-581817d583cc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###Our Dataset\nIn the workflow described below, we reference a database gdpr containing a sample dataset with 65,000,000 rows and as many distinct customer IDs, amounting to 3.228 GB of data."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6d4186ad-12b1-4a3d-9cf4-860e0565c9ab","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["######CUSTOMER TABLE\nThe schema of the customers table is as below \n````\n|-- c_customer_sk: integer (nullable = true)\n|-- c_customer_id: string (nullable = true)\n|-- c_current_cdemo_sk: integer (nullable = true)\n|-- c_current_hdemo_sk: integer (nullable = true)\n|-- c_current_addr_sk: integer (nullable = true)\n|-- c_first_shipto_date_sk: integer (nullable = true)\n|-- c_first_sales_date_sk: integer (nullable = true)\n|-- c_salutation: string (nullable = true)\n|-- c_first_name: string (nullable = true)\n|-- c_last_name: string (nullable = true)\n|-- c_preferred_cust_flag: string (nullable = true)\n|-- c_birth_day: integer (nullable = true)\n|-- c_birth_month: integer (nullable = true)\n|-- c_birth_year: integer (nullable = true)\n|-- c_birth_country: string (nullable = true)\n|-- c_login: string (nullable = true)\n|-- c_email_address: string (nullable = true)\n|-- c_last_review_date: string (nullable = true)\n````\n######Customer REQUESTs\nThe keys to be deleted (customer_id) as a part of the customer request are in `gdpr.customer_delete_keys` table,  are roughly about 10% (337.615 MB) of the Customer table\n\n````\n|-- c_customer_sk: integer (nullable = true)\n|-- c_customer_id: string (nullable = true)\n````"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2718e224-90a1-4346-9c8a-206a22d7d314","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### ![Delta Lake Logo Tiny](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) 1. Convert tables to Delta format"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0b4a60aa-c43e-4ede-954a-6472e4d5aea9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%scala\nspark.sql(\"CONVERT TO DELTA parquet.`dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5`\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"9069a18c-d1db-42db-b25a-0a46000704bf","inputWidgets":{},"title":"Convert Parquet to Delta"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"res19","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">The table you are trying to convert is already a delta table\nres19: org.apache.spark.sql.DataFrame = []\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The table you are trying to convert is already a delta table\nres19: org.apache.spark.sql.DataFrame = []\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nval csv_data = \"/databricks-datasets/flights/departuredelays.csv\"\n//\"/databricks-datasets/COVID/coronavirusdataset/PatientInfo.csv\" \nval homeDir = \"/Users/vini.jaiswal@databricks.com/demo\" \n\nval csvDF = (spark.read.format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(csv_data))\n\ncsvDF.write.mode(\"overwrite\").format(\"delta\").save(homeDir + \"/csv3-delta/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"4e1f258e-7641-4dc6-ac5c-60445d02bc21","inputWidgets":{},"title":"Convert non-Parquet to Delta"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"csvDF","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"date","type":"integer","nullable":true,"metadata":{}},{"name":"delay","type":"integer","nullable":true,"metadata":{}},{"name":"distance","type":"integer","nullable":true,"metadata":{}},{"name":"origin","type":"string","nullable":true,"metadata":{}},{"name":"destination","type":"string","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">csv_data: String = /databricks-datasets/flights/departuredelays.csv\nhomeDir: String = /Users/vini.jaiswal@databricks.com/demo\ncsvDF: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 3 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">csv_data: String = /databricks-datasets/flights/departuredelays.csv\nhomeDir: String = /Users/vini.jaiswal@databricks.com/demo\ncsvDF: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 3 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### ![Delta Lake Logo Tiny](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) 2. Delete from Delta Lake table\n\nThree different approaches:\n\n- Step 2a. Delete data using Delta DELETE statements without broadcasting\n- Step 2b. Delete data using Delta DELETE statements with broadcasting\n- Step 2c. Delete data using Delta DELETE statements, with broadcasting and Z-Ordering"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9742fb41-28df-44f0-a2bb-12a5a96012ca","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### Step 2a: Delete data using Delta `DELETE` statements"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eeff203e-6c58-432f-a138-5189219815b1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nset spark.sql.autoBroadcastJoinThreshold = -1;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"acf42fdc-7195-4f02-afca-a935e02343c1","inputWidgets":{},"title":"Turn broadcasting off for the first deletion"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["spark.sql.autoBroadcastJoinThreshold","-1"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"key","type":"\"string\"","metadata":"{}"},{"name":"value","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>spark.sql.autoBroadcastJoinThreshold</td><td>-1</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nspark.sql(\"\"\"\nSELECT count(*)\nFROM   delta.`dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5` \nWHERE  c_customer_id IN (SELECT c_customer_id \n                         FROM   gdpr.customer_delete_keys)\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"58682e18-fdf6-484d-b3a8-634961fd92b2","inputWidgets":{},"title":"**Exploring the matching records"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+\n|count(1)|\n+--------+\n| 6500000|\n+--------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+\ncount(1)|\n+--------+\n 6500000|\n+--------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nspark.sql(\"\"\"\nDELETE FROM delta.`dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5` AS t1\nWHERE  EXISTS (SELECT c_customer_id \n               FROM   gdpr.customer_delete_keys\n               WHERE  t1.c_customer_id = c_customer_id)\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"1e812d16-1cd7-437e-8c48-a28543bc00b2","inputWidgets":{},"title":"**Delete keys in our original table, no broadcasting"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"res1","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">res1: org.apache.spark.sql.DataFrame = []\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res1: org.apache.spark.sql.DataFrame = []\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nspark.sql(\"\"\"\nSELECT count(*)\nFROM   delta.`dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5` \nWHERE  c_customer_id IN (SELECT c_customer_id \n                         FROM   gdpr.customer_delete_keys)\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"d1710d21-0c4d-4772-b281-c8c2e84520f2","inputWidgets":{},"title":"Confirm delete action"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+\n|count(1)|\n+--------+\n|       0|\n+--------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+\ncount(1)|\n+--------+\n       0|\n+--------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Note**: Because we were able to easily `DELETE` the data, the above value should be `0`."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fb60ba7b-a86c-4fd7-b14e-acae0c0df4ce","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%scala\nval customer_t5 = \"dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5\"\nval df = spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(\"dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5\")\ndf.write.format(\"delta\").mode(\"overwrite\").save(customer_t5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"64bb7cd1-2c0a-4dc1-a595-8960ad636bed","inputWidgets":{},"title":"Recover the original state of the table - \"un-delete\" the entries"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"c_customer_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_customer_id","type":"string","nullable":true,"metadata":{}},{"name":"c_current_cdemo_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_current_hdemo_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_current_addr_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_first_shipto_date_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_first_sales_date_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_salutation","type":"string","nullable":true,"metadata":{}},{"name":"c_first_name","type":"string","nullable":true,"metadata":{}},{"name":"c_last_name","type":"string","nullable":true,"metadata":{}},{"name":"c_preferred_cust_flag","type":"string","nullable":true,"metadata":{}},{"name":"c_birth_day","type":"integer","nullable":true,"metadata":{}},{"name":"c_birth_month","type":"integer","nullable":true,"metadata":{}},{"name":"c_birth_year","type":"integer","nullable":true,"metadata":{}},{"name":"c_birth_country","type":"string","nullable":true,"metadata":{}},{"name":"c_login","type":"string","nullable":true,"metadata":{}},{"name":"c_email_address","type":"string","nullable":true,"metadata":{}},{"name":"c_last_review_date","type":"string","nullable":true,"metadata":{}}]},"tableIdentifier":"dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5"}],"data":"<div class=\"ansiout\">customer_t5: String = dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5\ndf: org.apache.spark.sql.DataFrame = [c_customer_sk: int, c_customer_id: string ... 16 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">customer_t5: String = dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5\ndf: org.apache.spark.sql.DataFrame = [c_customer_sk: int, c_customer_id: string ... 16 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Step 2b: Delete data using Delta `DELETE` statements, this time with broadcasting"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6c17654d-58bc-49ee-b154-c7921b6c618d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql set spark.sql.autoBroadcastJoinThreshold = 1024000000;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"b7d2fa1b-0bd5-43a0-9aa4-8e63dab784d3","inputWidgets":{},"title":"Enable broadcasting"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["spark.sql.autoBroadcastJoinThreshold","1024000000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"key","type":"\"string\"","metadata":"{}"},{"name":"value","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>spark.sql.autoBroadcastJoinThreshold</td><td>1024000000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nspark.sql(\"\"\"\nSELECT count(*)\nFROM   delta.`dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5` \nWHERE  c_customer_id IN (SELECT c_customer_id \n                         FROM   gdpr.customer_delete_keys)\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"aae31b57-e766-464d-94fd-c7395467a17d","inputWidgets":{},"title":"**Find the matching requests in customer table (Broadcast enabled)"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+\n|count(1)|\n+--------+\n| 6500000|\n+--------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+\ncount(1)|\n+--------+\n 6500000|\n+--------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nspark.sql(\"\"\"DELETE \nFROM   delta.`dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5` AS t1 \nWHERE  EXISTS \n       ( \n              SELECT c_customer_id \n              FROM   gdpr.customer_delete_keys\n              WHERE  t1.c_customer_id = c_customer_id)\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"b6ef92f0-73a5-4f18-b9a6-597c129cd6fe","inputWidgets":{},"title":"**Delta Delete (Broadcast Enabled)"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"res7","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">res7: org.apache.spark.sql.DataFrame = []\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res7: org.apache.spark.sql.DataFrame = []\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#####From the two runs above; you can easily observe that being able to broadcast helps the delete process in determining which files are to be rewritten."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"40cdffd3-5511-438a-9467-9afc9134cb1f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%scala\nval df = spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(\"dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5\")\ndf.write.format(\"delta\").mode(\"overwrite\").save(customer_t5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"3d633f99-0ca3-4148-9db1-2a93034dc5bd","inputWidgets":{},"title":"Recover the table"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"c_customer_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_customer_id","type":"string","nullable":true,"metadata":{}},{"name":"c_current_cdemo_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_current_hdemo_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_current_addr_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_first_shipto_date_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_first_sales_date_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_salutation","type":"string","nullable":true,"metadata":{}},{"name":"c_first_name","type":"string","nullable":true,"metadata":{}},{"name":"c_last_name","type":"string","nullable":true,"metadata":{}},{"name":"c_preferred_cust_flag","type":"string","nullable":true,"metadata":{}},{"name":"c_birth_day","type":"integer","nullable":true,"metadata":{}},{"name":"c_birth_month","type":"integer","nullable":true,"metadata":{}},{"name":"c_birth_year","type":"integer","nullable":true,"metadata":{}},{"name":"c_birth_country","type":"string","nullable":true,"metadata":{}},{"name":"c_login","type":"string","nullable":true,"metadata":{}},{"name":"c_email_address","type":"string","nullable":true,"metadata":{}},{"name":"c_last_review_date","type":"string","nullable":true,"metadata":{}}]},"tableIdentifier":"dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5"}],"data":"<div class=\"ansiout\">df: org.apache.spark.sql.DataFrame = [c_customer_sk: int, c_customer_id: string ... 16 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">df: org.apache.spark.sql.DataFrame = [c_customer_sk: int, c_customer_id: string ... 16 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Step 2c: Delete data using Delta `DELETE` statements, with broadcasting and Z-Ordering"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fce60fb3-5dcf-4dda-8c28-b3d4fdbd0fc3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%scala\nspark.sql(\"optimize delta.`dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5` zorder by c_customer_id\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"8e1cdf05-9994-4fb1-815a-c2e8ce28c407","inputWidgets":{},"title":"Z-Order the customers table on customer_id"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"res11","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"path","type":"string","nullable":true,"metadata":{}},{"name":"metrics","type":{"type":"struct","fields":[{"name":"numFilesAdded","type":"long","nullable":false,"metadata":{}},{"name":"numFilesRemoved","type":"long","nullable":false,"metadata":{}},{"name":"filesAdded","type":{"type":"struct","fields":[{"name":"min","type":"long","nullable":true,"metadata":{}},{"name":"max","type":"long","nullable":true,"metadata":{}},{"name":"avg","type":"double","nullable":false,"metadata":{}},{"name":"totalFiles","type":"long","nullable":false,"metadata":{}},{"name":"totalSize","type":"long","nullable":false,"metadata":{}}]},"nullable":true,"metadata":{}},{"name":"filesRemoved","type":{"type":"struct","fields":[{"name":"min","type":"long","nullable":true,"metadata":{}},{"name":"max","type":"long","nullable":true,"metadata":{}},{"name":"avg","type":"double","nullable":false,"metadata":{}},{"name":"totalFiles","type":"long","nullable":false,"metadata":{}},{"name":"totalSize","type":"long","nullable":false,"metadata":{}}]},"nullable":true,"metadata":{}},{"name":"partitionsOptimized","type":"long","nullable":false,"metadata":{}},{"name":"zOrderStats","type":{"type":"struct","fields":[{"name":"strategyName","type":"string","nullable":true,"metadata":{}},{"name":"inputCubeFiles","type":{"type":"struct","fields":[{"name":"num","type":"long","nullable":false,"metadata":{}},{"name":"size","type":"long","nullable":false,"metadata":{}}]},"nullable":true,"metadata":{}},{"name":"inputOtherFiles","type":{"type":"struct","fields":[{"name":"num","type":"long","nullable":false,"metadata":{}},{"name":"size","type":"long","nullable":false,"metadata":{}}]},"nullable":true,"metadata":{}},{"name":"inputNumCubes","type":"long","nullable":false,"metadata":{}},{"name":"mergedFiles","type":{"type":"struct","fields":[{"name":"num","type":"long","nullable":false,"metadata":{}},{"name":"size","type":"long","nullable":false,"metadata":{}}]},"nullable":true,"metadata":{}},{"name":"mergedNumCubes","type":"long","nullable":false,"metadata":{}}]},"nullable":true,"metadata":{}},{"name":"numBatches","type":"long","nullable":false,"metadata":{}}]},"nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">res11: org.apache.spark.sql.DataFrame = [path: string, metrics: struct&lt;numFilesAdded: bigint, numFilesRemoved: bigint ... 5 more fields&gt;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res11: org.apache.spark.sql.DataFrame = [path: string, metrics: struct&lt;numFilesAdded: bigint, numFilesRemoved: bigint ... 5 more fields&gt;]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nspark.sql(\"select count(1) from delta.`dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5` where c_customer_id in (select c_customer_id from gdpr.customer_delete_keys)\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"5aecd59b-47b2-4687-9de2-7c62b3e6acab","inputWidgets":{},"title":"Find the matching requests in z-ordered table"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+\n|count(1)|\n+--------+\n| 6500000|\n+--------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+\ncount(1)|\n+--------+\n 6500000|\n+--------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nspark.sql(\"\"\"DELETE \nFROM   delta.`dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5` AS t1 \nWHERE  EXISTS \n       ( \n              SELECT c_customer_id \n              FROM   gdpr.customer_delete_keys\n              WHERE  t1.c_customer_id = c_customer_id)\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"6c33ba37-fbb7-4ca6-b06e-5a2984bbde15","inputWidgets":{},"title":"Delete Operation (zorder helped by rewriting fewer files) "}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"res14","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">res14: org.apache.spark.sql.DataFrame = []\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res14: org.apache.spark.sql.DataFrame = []\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nval df = spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(\"dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5\")\ndf.write.format(\"delta\").mode(\"overwrite\").save(customer_t5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"88040ec2-de48-4ee2-a801-0f8effe4f68f","inputWidgets":{},"title":"Recover Table"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"c_customer_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_customer_id","type":"string","nullable":true,"metadata":{}},{"name":"c_current_cdemo_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_current_hdemo_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_current_addr_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_first_shipto_date_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_first_sales_date_sk","type":"integer","nullable":true,"metadata":{}},{"name":"c_salutation","type":"string","nullable":true,"metadata":{}},{"name":"c_first_name","type":"string","nullable":true,"metadata":{}},{"name":"c_last_name","type":"string","nullable":true,"metadata":{}},{"name":"c_preferred_cust_flag","type":"string","nullable":true,"metadata":{}},{"name":"c_birth_day","type":"integer","nullable":true,"metadata":{}},{"name":"c_birth_month","type":"integer","nullable":true,"metadata":{}},{"name":"c_birth_year","type":"integer","nullable":true,"metadata":{}},{"name":"c_birth_country","type":"string","nullable":true,"metadata":{}},{"name":"c_login","type":"string","nullable":true,"metadata":{}},{"name":"c_email_address","type":"string","nullable":true,"metadata":{}},{"name":"c_last_review_date","type":"string","nullable":true,"metadata":{}}]},"tableIdentifier":"dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5"}],"data":"<div class=\"ansiout\">df: org.apache.spark.sql.DataFrame = [c_customer_sk: int, c_customer_id: string ... 16 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">df: org.apache.spark.sql.DataFrame = [c_customer_sk: int, c_customer_id: string ... 16 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Audit Transactional logs with Delta Lake"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8332e711-22c2-48a9-aee6-35acf06d6ae0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%scala\ndisplay(spark.sql(\"DESCRIBE HISTORY delta.`dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t2`\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5ec4ed0a-7082-4be3-8160-6c677e967e7e","inputWidgets":{},"title":"Audit & Transaction Logs"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[21,"2020-06-18T03:28:05.000+0000","100708","vini.jaiswal@databricks.com","OPTIMIZE",{"predicate":"[]","zOrderBy":"[\"c_customer_id\"]","batchId":"0","auto":"false"},null,["6718320"],"0618-012132-spiky36",20,"SnapshotIsolation",false,{"numRemovedFiles":"208","numRemovedBytes":"3194861916","p25FileSize":"540468009","minFileSize":"461126371","numAddedFiles":"6","maxFileSize":"542929260","p75FileSize":"542859315","p50FileSize":"541858629","numAddedBytes":"3170428305"},null,null],[20,"2020-06-18T03:21:57.000+0000","100708","vini.jaiswal@databricks.com","DELETE",{"predicate":"[\"exists(t1.`c_customer_id`)\"]"},null,["6718320"],"0618-012132-spiky36",19,"WriteSerializable",false,{"numRemovedFiles":"23","numDeletedRows":"6500000","numAddedFiles":"200","numCopiedRows":"42250002"},null,null],[19,"2020-05-07T16:48:32.000+0000","100708","vini.jaiswal@databricks.com","WRITE",{"mode":"Overwrite","partitionBy":"[]"},null,["6718320"],"0506-044457-pump2",18,"WriteSerializable",false,{"numFiles":"31","numOutputBytes":"3453647006","numOutputRows":"65000000","numParts":"0"},null,null],[18,"2020-05-07T16:47:05.000+0000","100708","vini.jaiswal@databricks.com","DELETE",{"predicate":"[\"exists(t1.`c_customer_id`)\"]"},null,["6718320"],"0506-044457-pump2",17,"WriteSerializable",false,{"numTotalRows":"65000000","numFiles":"26","numRemovedFiles":"9","numCopiedRows":"58500000","numDeletedRows":"6500000","numOutputRows":"57311759","numParts":"0","numOutputBytes":"3041064134","numAddedFiles":"26"},null,null],[17,"2020-05-07T16:45:04.000+0000","100708","vini.jaiswal@databricks.com","OPTIMIZE",{"predicate":"[]","zOrderBy":"[\"c_customer_id\"]","batchId":"0","auto":"false"},null,["6718320"],"0506-044457-pump2",16,"SnapshotIsolation",false,{"numFiles":"9","numRemovedFiles":"31","numRemovedBytes":"3453647007","p25FileSize":"117798102","minFileSize":"63127260","numOutputRows":"65000000","numParts":"0","numOutputBytes":"3448893504","numAddedFiles":"9","maxFileSize":"530630703","p75FileSize":"530592677","p50FileSize":"530560834","numAddedBytes":"3448893505"},null,null],[16,"2020-05-07T16:39:09.000+0000","100708","vini.jaiswal@databricks.com","WRITE",{"mode":"Overwrite","partitionBy":"[]"},null,["6718320"],"0506-044457-pump2",15,"WriteSerializable",false,{"numFiles":"31","numOutputBytes":"3453647006","numOutputRows":"65000000","numParts":"0"},null,null],[15,"2020-05-07T16:37:39.000+0000","100708","vini.jaiswal@databricks.com","DELETE",{"predicate":"[\"exists(t1.`c_customer_id`)\"]"},null,["6718320"],"0506-044457-pump2",14,"WriteSerializable",false,{"numTotalRows":"65000000","numFiles":"26","numRemovedFiles":"9","numCopiedRows":"58500000","numDeletedRows":"6500000","numOutputRows":"57314981","numParts":"0","numOutputBytes":"3041396932","numAddedFiles":"26"},null,null],[14,"2020-05-07T16:35:35.000+0000","100708","vini.jaiswal@databricks.com","OPTIMIZE",{"predicate":"[]","zOrderBy":"[\"c_customer_id\"]","batchId":"0","auto":"false"},null,["6718320"],"0506-044457-pump2",13,"SnapshotIsolation",false,{"numFiles":"9","numRemovedFiles":"31","numRemovedBytes":"3453647007","p25FileSize":"136690928","minFileSize":"62942854","numOutputRows":"65000000","numParts":"0","numOutputBytes":"3449127978","numAddedFiles":"9","maxFileSize":"530668371","p75FileSize":"530603594","p50FileSize":"530562862","numAddedBytes":"3449127979"},null,null],[13,"2020-05-07T16:31:22.000+0000","100708","vini.jaiswal@databricks.com","WRITE",{"mode":"Overwrite","partitionBy":"[]"},null,["6718320"],"0506-044457-pump2",12,"WriteSerializable",false,{"numFiles":"31","numOutputBytes":"3453647006","numOutputRows":"65000000","numParts":"0"},null,null],[12,"2020-05-07T16:29:48.000+0000","100708","vini.jaiswal@databricks.com","DELETE",{"predicate":"[\"exists(t1.`c_customer_id`)\"]"},null,["6718320"],"0506-044457-pump2",11,"WriteSerializable",false,{"numTotalRows":"65000000","numFiles":"23","numRemovedFiles":"31","numCopiedRows":"58500000","numDeletedRows":"6500000","numOutputRows":"42250002","numParts":"0","numOutputBytes":"2245108004","numAddedFiles":"23"},null,null],[11,"2020-05-07T16:28:07.000+0000","100708","vini.jaiswal@databricks.com","WRITE",{"mode":"Overwrite","partitionBy":"[]"},null,["6718320"],"0506-044457-pump2",10,"WriteSerializable",false,{"numFiles":"31","numOutputBytes":"3453647006","numOutputRows":"65000000","numParts":"0"},null,null]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"version","type":"\"long\"","metadata":"{}"},{"name":"timestamp","type":"\"timestamp\"","metadata":"{}"},{"name":"userId","type":"\"string\"","metadata":"{}"},{"name":"userName","type":"\"string\"","metadata":"{}"},{"name":"operation","type":"\"string\"","metadata":"{}"},{"name":"operationParameters","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"job","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"notebook","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"clusterId","type":"\"string\"","metadata":"{}"},{"name":"readVersion","type":"\"long\"","metadata":"{}"},{"name":"isolationLevel","type":"\"string\"","metadata":"{}"},{"name":"isBlindAppend","type":"\"boolean\"","metadata":"{}"},{"name":"operationMetrics","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"userMetadata","type":"\"string\"","metadata":"{}"},{"name":"engineInfo","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>21</td><td>2020-06-18T03:28:05.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>OPTIMIZE</td><td>Map(predicate -> [], zOrderBy -> [\"c_customer_id\"], batchId -> 0, auto -> false)</td><td>null</td><td>List(6718320)</td><td>0618-012132-spiky36</td><td>20</td><td>SnapshotIsolation</td><td>false</td><td>Map(numRemovedFiles -> 208, numRemovedBytes -> 3194861916, p25FileSize -> 540468009, minFileSize -> 461126371, numAddedFiles -> 6, maxFileSize -> 542929260, p75FileSize -> 542859315, p50FileSize -> 541858629, numAddedBytes -> 3170428305)</td><td>null</td><td>null</td></tr><tr><td>20</td><td>2020-06-18T03:21:57.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>DELETE</td><td>Map(predicate -> [\"exists(t1.`c_customer_id`)\"])</td><td>null</td><td>List(6718320)</td><td>0618-012132-spiky36</td><td>19</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 23, numDeletedRows -> 6500000, numAddedFiles -> 200, numCopiedRows -> 42250002)</td><td>null</td><td>null</td></tr><tr><td>19</td><td>2020-05-07T16:48:32.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>18</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 31, numOutputBytes -> 3453647006, numOutputRows -> 65000000, numParts -> 0)</td><td>null</td><td>null</td></tr><tr><td>18</td><td>2020-05-07T16:47:05.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>DELETE</td><td>Map(predicate -> [\"exists(t1.`c_customer_id`)\"])</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>17</td><td>WriteSerializable</td><td>false</td><td>Map(numTotalRows -> 65000000, numFiles -> 26, numRemovedFiles -> 9, numCopiedRows -> 58500000, numDeletedRows -> 6500000, numOutputRows -> 57311759, numParts -> 0, numOutputBytes -> 3041064134, numAddedFiles -> 26)</td><td>null</td><td>null</td></tr><tr><td>17</td><td>2020-05-07T16:45:04.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>OPTIMIZE</td><td>Map(predicate -> [], zOrderBy -> [\"c_customer_id\"], batchId -> 0, auto -> false)</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>16</td><td>SnapshotIsolation</td><td>false</td><td>Map(numFiles -> 9, numRemovedFiles -> 31, numRemovedBytes -> 3453647007, p25FileSize -> 117798102, minFileSize -> 63127260, numOutputRows -> 65000000, numParts -> 0, numOutputBytes -> 3448893504, numAddedFiles -> 9, maxFileSize -> 530630703, p75FileSize -> 530592677, p50FileSize -> 530560834, numAddedBytes -> 3448893505)</td><td>null</td><td>null</td></tr><tr><td>16</td><td>2020-05-07T16:39:09.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>15</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 31, numOutputBytes -> 3453647006, numOutputRows -> 65000000, numParts -> 0)</td><td>null</td><td>null</td></tr><tr><td>15</td><td>2020-05-07T16:37:39.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>DELETE</td><td>Map(predicate -> [\"exists(t1.`c_customer_id`)\"])</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>14</td><td>WriteSerializable</td><td>false</td><td>Map(numTotalRows -> 65000000, numFiles -> 26, numRemovedFiles -> 9, numCopiedRows -> 58500000, numDeletedRows -> 6500000, numOutputRows -> 57314981, numParts -> 0, numOutputBytes -> 3041396932, numAddedFiles -> 26)</td><td>null</td><td>null</td></tr><tr><td>14</td><td>2020-05-07T16:35:35.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>OPTIMIZE</td><td>Map(predicate -> [], zOrderBy -> [\"c_customer_id\"], batchId -> 0, auto -> false)</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>13</td><td>SnapshotIsolation</td><td>false</td><td>Map(numFiles -> 9, numRemovedFiles -> 31, numRemovedBytes -> 3453647007, p25FileSize -> 136690928, minFileSize -> 62942854, numOutputRows -> 65000000, numParts -> 0, numOutputBytes -> 3449127978, numAddedFiles -> 9, maxFileSize -> 530668371, p75FileSize -> 530603594, p50FileSize -> 530562862, numAddedBytes -> 3449127979)</td><td>null</td><td>null</td></tr><tr><td>13</td><td>2020-05-07T16:31:22.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>12</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 31, numOutputBytes -> 3453647006, numOutputRows -> 65000000, numParts -> 0)</td><td>null</td><td>null</td></tr><tr><td>12</td><td>2020-05-07T16:29:48.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>DELETE</td><td>Map(predicate -> [\"exists(t1.`c_customer_id`)\"])</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>11</td><td>WriteSerializable</td><td>false</td><td>Map(numTotalRows -> 65000000, numFiles -> 23, numRemovedFiles -> 31, numCopiedRows -> 58500000, numDeletedRows -> 6500000, numOutputRows -> 42250002, numParts -> 0, numOutputBytes -> 2245108004, numAddedFiles -> 23)</td><td>null</td><td>null</td></tr><tr><td>11</td><td>2020-05-07T16:28:07.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>10</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 31, numOutputBytes -> 3453647006, numOutputRows -> 65000000, numParts -> 0)</td><td>null</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) 4. Vacuum Delta Lake tables\n\nBy default, `vacuum()` retains all the data needed for the last 7 days. For this example, since this table does not have 7 days worth of history, we will retain 0 hours, which means to only keep the latest state of the table."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9584c06c-1766-488a-b49c-c37d7fcdff37","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%scala\n//setting up for vacuum\nimport io.delta.tables._\nval delta_path = \"dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5\"\n\nval deltaTable = DeltaTable.forPath(spark, delta_path)\ndisplay(deltaTable.history())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"64da13a4-bb51-4a5c-81f6-6a1319c5a79d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[21,"2020-06-18T03:28:05.000+0000","100708","vini.jaiswal@databricks.com","OPTIMIZE",{"predicate":"[]","zOrderBy":"[\"c_customer_id\"]","batchId":"0","auto":"false"},null,["6718320"],"0618-012132-spiky36",20,"SnapshotIsolation",false,{"numRemovedFiles":"208","numRemovedBytes":"3194861916","p25FileSize":"540468009","minFileSize":"461126371","numAddedFiles":"6","maxFileSize":"542929260","p75FileSize":"542859315","p50FileSize":"541858629","numAddedBytes":"3170428305"}],[20,"2020-06-18T03:21:57.000+0000","100708","vini.jaiswal@databricks.com","DELETE",{"predicate":"[\"exists(t1.`c_customer_id`)\"]"},null,["6718320"],"0618-012132-spiky36",19,"WriteSerializable",false,{"numRemovedFiles":"23","numDeletedRows":"6500000","numAddedFiles":"200","numCopiedRows":"42250002"}],[19,"2020-05-07T16:48:32.000+0000","100708","vini.jaiswal@databricks.com","WRITE",{"mode":"Overwrite","partitionBy":"[]"},null,["6718320"],"0506-044457-pump2",18,"WriteSerializable",false,{"numFiles":"31","numOutputBytes":"3453647006","numOutputRows":"65000000","numParts":"0"}],[18,"2020-05-07T16:47:05.000+0000","100708","vini.jaiswal@databricks.com","DELETE",{"predicate":"[\"exists(t1.`c_customer_id`)\"]"},null,["6718320"],"0506-044457-pump2",17,"WriteSerializable",false,{"numTotalRows":"65000000","numFiles":"26","numRemovedFiles":"9","numCopiedRows":"58500000","numDeletedRows":"6500000","numOutputRows":"57311759","numParts":"0","numOutputBytes":"3041064134","numAddedFiles":"26"}],[17,"2020-05-07T16:45:04.000+0000","100708","vini.jaiswal@databricks.com","OPTIMIZE",{"predicate":"[]","zOrderBy":"[\"c_customer_id\"]","batchId":"0","auto":"false"},null,["6718320"],"0506-044457-pump2",16,"SnapshotIsolation",false,{"numFiles":"9","numRemovedFiles":"31","numRemovedBytes":"3453647007","p25FileSize":"117798102","minFileSize":"63127260","numOutputRows":"65000000","numParts":"0","numOutputBytes":"3448893504","numAddedFiles":"9","maxFileSize":"530630703","p75FileSize":"530592677","p50FileSize":"530560834","numAddedBytes":"3448893505"}],[16,"2020-05-07T16:39:09.000+0000","100708","vini.jaiswal@databricks.com","WRITE",{"mode":"Overwrite","partitionBy":"[]"},null,["6718320"],"0506-044457-pump2",15,"WriteSerializable",false,{"numFiles":"31","numOutputBytes":"3453647006","numOutputRows":"65000000","numParts":"0"}],[15,"2020-05-07T16:37:39.000+0000","100708","vini.jaiswal@databricks.com","DELETE",{"predicate":"[\"exists(t1.`c_customer_id`)\"]"},null,["6718320"],"0506-044457-pump2",14,"WriteSerializable",false,{"numTotalRows":"65000000","numFiles":"26","numRemovedFiles":"9","numCopiedRows":"58500000","numDeletedRows":"6500000","numOutputRows":"57314981","numParts":"0","numOutputBytes":"3041396932","numAddedFiles":"26"}],[14,"2020-05-07T16:35:35.000+0000","100708","vini.jaiswal@databricks.com","OPTIMIZE",{"predicate":"[]","zOrderBy":"[\"c_customer_id\"]","batchId":"0","auto":"false"},null,["6718320"],"0506-044457-pump2",13,"SnapshotIsolation",false,{"numFiles":"9","numRemovedFiles":"31","numRemovedBytes":"3453647007","p25FileSize":"136690928","minFileSize":"62942854","numOutputRows":"65000000","numParts":"0","numOutputBytes":"3449127978","numAddedFiles":"9","maxFileSize":"530668371","p75FileSize":"530603594","p50FileSize":"530562862","numAddedBytes":"3449127979"}],[13,"2020-05-07T16:31:22.000+0000","100708","vini.jaiswal@databricks.com","WRITE",{"mode":"Overwrite","partitionBy":"[]"},null,["6718320"],"0506-044457-pump2",12,"WriteSerializable",false,{"numFiles":"31","numOutputBytes":"3453647006","numOutputRows":"65000000","numParts":"0"}],[12,"2020-05-07T16:29:48.000+0000","100708","vini.jaiswal@databricks.com","DELETE",{"predicate":"[\"exists(t1.`c_customer_id`)\"]"},null,["6718320"],"0506-044457-pump2",11,"WriteSerializable",false,{"numTotalRows":"65000000","numFiles":"23","numRemovedFiles":"31","numCopiedRows":"58500000","numDeletedRows":"6500000","numOutputRows":"42250002","numParts":"0","numOutputBytes":"2245108004","numAddedFiles":"23"}],[11,"2020-05-07T16:28:07.000+0000","100708","vini.jaiswal@databricks.com","WRITE",{"mode":"Overwrite","partitionBy":"[]"},null,["6718320"],"0506-044457-pump2",10,"WriteSerializable",false,{"numFiles":"31","numOutputBytes":"3453647006","numOutputRows":"65000000","numParts":"0"}]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"version","type":"\"long\"","metadata":"{}"},{"name":"timestamp","type":"\"timestamp\"","metadata":"{}"},{"name":"userId","type":"\"string\"","metadata":"{}"},{"name":"userName","type":"\"string\"","metadata":"{}"},{"name":"operation","type":"\"string\"","metadata":"{}"},{"name":"operationParameters","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"job","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"notebook","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"clusterId","type":"\"string\"","metadata":"{}"},{"name":"readVersion","type":"\"long\"","metadata":"{}"},{"name":"isolationLevel","type":"\"string\"","metadata":"{}"},{"name":"isBlindAppend","type":"\"boolean\"","metadata":"{}"},{"name":"operationMetrics","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th></tr></thead><tbody><tr><td>21</td><td>2020-06-18T03:28:05.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>OPTIMIZE</td><td>Map(predicate -> [], zOrderBy -> [\"c_customer_id\"], batchId -> 0, auto -> false)</td><td>null</td><td>List(6718320)</td><td>0618-012132-spiky36</td><td>20</td><td>SnapshotIsolation</td><td>false</td><td>Map(numRemovedFiles -> 208, numRemovedBytes -> 3194861916, p25FileSize -> 540468009, minFileSize -> 461126371, numAddedFiles -> 6, maxFileSize -> 542929260, p75FileSize -> 542859315, p50FileSize -> 541858629, numAddedBytes -> 3170428305)</td></tr><tr><td>20</td><td>2020-06-18T03:21:57.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>DELETE</td><td>Map(predicate -> [\"exists(t1.`c_customer_id`)\"])</td><td>null</td><td>List(6718320)</td><td>0618-012132-spiky36</td><td>19</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 23, numDeletedRows -> 6500000, numAddedFiles -> 200, numCopiedRows -> 42250002)</td></tr><tr><td>19</td><td>2020-05-07T16:48:32.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>18</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 31, numOutputBytes -> 3453647006, numOutputRows -> 65000000, numParts -> 0)</td></tr><tr><td>18</td><td>2020-05-07T16:47:05.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>DELETE</td><td>Map(predicate -> [\"exists(t1.`c_customer_id`)\"])</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>17</td><td>WriteSerializable</td><td>false</td><td>Map(numTotalRows -> 65000000, numFiles -> 26, numRemovedFiles -> 9, numCopiedRows -> 58500000, numDeletedRows -> 6500000, numOutputRows -> 57311759, numParts -> 0, numOutputBytes -> 3041064134, numAddedFiles -> 26)</td></tr><tr><td>17</td><td>2020-05-07T16:45:04.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>OPTIMIZE</td><td>Map(predicate -> [], zOrderBy -> [\"c_customer_id\"], batchId -> 0, auto -> false)</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>16</td><td>SnapshotIsolation</td><td>false</td><td>Map(numFiles -> 9, numRemovedFiles -> 31, numRemovedBytes -> 3453647007, p25FileSize -> 117798102, minFileSize -> 63127260, numOutputRows -> 65000000, numParts -> 0, numOutputBytes -> 3448893504, numAddedFiles -> 9, maxFileSize -> 530630703, p75FileSize -> 530592677, p50FileSize -> 530560834, numAddedBytes -> 3448893505)</td></tr><tr><td>16</td><td>2020-05-07T16:39:09.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>15</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 31, numOutputBytes -> 3453647006, numOutputRows -> 65000000, numParts -> 0)</td></tr><tr><td>15</td><td>2020-05-07T16:37:39.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>DELETE</td><td>Map(predicate -> [\"exists(t1.`c_customer_id`)\"])</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>14</td><td>WriteSerializable</td><td>false</td><td>Map(numTotalRows -> 65000000, numFiles -> 26, numRemovedFiles -> 9, numCopiedRows -> 58500000, numDeletedRows -> 6500000, numOutputRows -> 57314981, numParts -> 0, numOutputBytes -> 3041396932, numAddedFiles -> 26)</td></tr><tr><td>14</td><td>2020-05-07T16:35:35.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>OPTIMIZE</td><td>Map(predicate -> [], zOrderBy -> [\"c_customer_id\"], batchId -> 0, auto -> false)</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>13</td><td>SnapshotIsolation</td><td>false</td><td>Map(numFiles -> 9, numRemovedFiles -> 31, numRemovedBytes -> 3453647007, p25FileSize -> 136690928, minFileSize -> 62942854, numOutputRows -> 65000000, numParts -> 0, numOutputBytes -> 3449127978, numAddedFiles -> 9, maxFileSize -> 530668371, p75FileSize -> 530603594, p50FileSize -> 530562862, numAddedBytes -> 3449127979)</td></tr><tr><td>13</td><td>2020-05-07T16:31:22.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>12</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 31, numOutputBytes -> 3453647006, numOutputRows -> 65000000, numParts -> 0)</td></tr><tr><td>12</td><td>2020-05-07T16:29:48.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>DELETE</td><td>Map(predicate -> [\"exists(t1.`c_customer_id`)\"])</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>11</td><td>WriteSerializable</td><td>false</td><td>Map(numTotalRows -> 65000000, numFiles -> 23, numRemovedFiles -> 31, numCopiedRows -> 58500000, numDeletedRows -> 6500000, numOutputRows -> 42250002, numParts -> 0, numOutputBytes -> 2245108004, numAddedFiles -> 23)</td></tr><tr><td>11</td><td>2020-05-07T16:28:07.000+0000</td><td>100708</td><td>vini.jaiswal@databricks.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(6718320)</td><td>0506-044457-pump2</td><td>10</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 31, numOutputBytes -> 3453647006, numOutputRows -> 65000000, numParts -> 0)</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n//performing vacuum\nspark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled = false\")\ndeltaTable.vacuum(retentionHours = 0)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe1b876a-3be0-4558-9ad7-c51206e746d9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"res21","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">Deleted 1259 files and directories in a total of 1 directories.\nres21: org.apache.spark.sql.DataFrame = []\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Deleted 1259 files and directories in a total of 1 directories.\nres21: org.apache.spark.sql.DataFrame = []\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nval df = spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(\"dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5\")\ndf.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t5\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"38d947e9-4783-485d-a0b0-29fa9782e29f","inputWidgets":{},"title":"This should fail "}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\">\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:201)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$$anonfun$writeFiles$1$$anonfun$apply$1.apply(TransactionalWriteEdge.scala:169)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$$anonfun$writeFiles$1$$anonfun$apply$1.apply(TransactionalWriteEdge.scala:133)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1$$anonfun$apply$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:217)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:74)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:169)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$$anonfun$writeFiles$1.apply(TransactionalWriteEdge.scala:133)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$$anonfun$writeFiles$1.apply(TransactionalWriteEdge.scala:90)\n\tat com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:430)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:18)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:18)\n\tat com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:411)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:18)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:55)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)\n\tat com.databricks.spark.util.UsageLogger$class.recordOperation(UsageLogger.scala:67)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:67)\n\tat com.databricks.spark.util.UsageLogging$class.recordOperation(UsageLogger.scala:342)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:83)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging$class.recordDeltaOperation(DeltaLogging.scala:108)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:83)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$class.writeFiles(TransactionalWriteEdge.scala:90)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:83)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWrite$class.writeFiles(TransactionalWrite.scala:110)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:83)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:111)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1$$anonfun$apply$1.apply(WriteIntoDelta.scala:71)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1$$anonfun$apply$1.apply(WriteIntoDelta.scala:70)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:207)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:70)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:69)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:832)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.run(WriteIntoDelta.scala:69)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:128)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:150)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:191)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:187)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:117)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1$$anonfun$apply$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:217)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:74)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:169)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:307)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:236)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:2)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:56)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:58)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:60)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:62)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:64)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:66)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw.&lt;init&gt;(command-6765358:68)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw.&lt;init&gt;(command-6765358:70)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw.&lt;init&gt;(command-6765358:72)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read.&lt;init&gt;(command-6765358:74)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$.&lt;init&gt;(command-6765358:78)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$.&lt;clinit&gt;(command-6765358)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$eval$.$print(&lt;notebook&gt;:6)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\n\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:215)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:714)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:667)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:396)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:373)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:645)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:640)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 25 in stage 201.0 failed 4 times, most recent failure: Lost task 25.3 in stage 201.0 (TID 4519, 10.0.234.148, executor 1): com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@databricks.com/demo/customer_t2/part-00118-58d2e7ba-a139-44ce-b220-c6c150356875-c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.databricks.com/delta/delta-intro.html#frequently-asked-questions\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:361)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:327)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anonfun$prepareNextFile$1.apply(FileScanRDD.scala:493)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anonfun$prepareNextFile$1.apply(FileScanRDD.scala:481)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply$mcV$sp(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper$class.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t2/part-00118-58d2e7ba-a139-44ce-b220-c6c150356875-c000.snappy.parquet\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1$$anonfun$apply$15.apply(DatabricksFileSystemV2.scala:773)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1$$anonfun$apply$15.apply(DatabricksFileSystemV2.scala:759)\n\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:119)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1.apply(DatabricksFileSystemV2.scala:759)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1.apply(DatabricksFileSystemV2.scala:759)\n\tat com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:430)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:411)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:758)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.getFileStatus(FileSystemWithMetrics.scala:295)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:405)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:128)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:478)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:390)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:311)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2582)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:170)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$$anonfun$writeFiles$1$$anonfun$apply$1.apply(TransactionalWriteEdge.scala:169)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$$anonfun$writeFiles$1$$anonfun$apply$1.apply(TransactionalWriteEdge.scala:133)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1$$anonfun$apply$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:217)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:74)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:169)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$$anonfun$writeFiles$1.apply(TransactionalWriteEdge.scala:133)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$$anonfun$writeFiles$1.apply(TransactionalWriteEdge.scala:90)\n\tat com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:430)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:18)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:18)\n\tat com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:411)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:18)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:55)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)\n\tat com.databricks.spark.util.UsageLogger$class.recordOperation(UsageLogger.scala:67)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:67)\n\tat com.databricks.spark.util.UsageLogging$class.recordOperation(UsageLogger.scala:342)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:83)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging$class.recordDeltaOperation(DeltaLogging.scala:108)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:83)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$class.writeFiles(TransactionalWriteEdge.scala:90)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:83)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWrite$class.writeFiles(TransactionalWrite.scala:110)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:83)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:111)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1$$anonfun$apply$1.apply(WriteIntoDelta.scala:71)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1$$anonfun$apply$1.apply(WriteIntoDelta.scala:70)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:207)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:70)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:69)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:832)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.run(WriteIntoDelta.scala:69)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:128)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:150)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:191)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:187)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:117)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1$$anonfun$apply$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:217)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:74)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:169)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:307)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:236)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:2)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:56)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:58)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:60)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:62)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:64)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:66)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw.&lt;init&gt;(command-6765358:68)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw.&lt;init&gt;(command-6765358:70)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw.&lt;init&gt;(command-6765358:72)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read.&lt;init&gt;(command-6765358:74)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$.&lt;init&gt;(command-6765358:78)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$.&lt;clinit&gt;(command-6765358)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$eval$.$print(&lt;notebook&gt;:6)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\n\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:215)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:714)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:667)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:396)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:373)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:645)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:640)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@databricks.com/demo/customer_t2/part-00118-58d2e7ba-a139-44ce-b220-c6c150356875-c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.databricks.com/delta/delta-intro.html#frequently-asked-questions\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:361)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:327)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anonfun$prepareNextFile$1.apply(FileScanRDD.scala:493)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anonfun$prepareNextFile$1.apply(FileScanRDD.scala:481)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply$mcV$sp(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper$class.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t2/part-00118-58d2e7ba-a139-44ce-b220-c6c150356875-c000.snappy.parquet\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1$$anonfun$apply$15.apply(DatabricksFileSystemV2.scala:773)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1$$anonfun$apply$15.apply(DatabricksFileSystemV2.scala:759)\n\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:119)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1.apply(DatabricksFileSystemV2.scala:759)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1.apply(DatabricksFileSystemV2.scala:759)\n\tat com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:430)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:411)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:758)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.getFileStatus(FileSystemWithMetrics.scala:295)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:405)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:128)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:478)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:390)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:311)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anonfun$prepareNextFile$1.apply(FileScanRDD.scala:493)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anonfun$prepareNextFile$1.apply(FileScanRDD.scala:481)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply$mcV$sp(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper$class.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)</div>","errorSummary":"org.apache.spark.SparkException: Job aborted.","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:201)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$$anonfun$writeFiles$1$$anonfun$apply$1.apply(TransactionalWriteEdge.scala:169)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$$anonfun$writeFiles$1$$anonfun$apply$1.apply(TransactionalWriteEdge.scala:133)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1$$anonfun$apply$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:217)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:74)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:169)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$$anonfun$writeFiles$1.apply(TransactionalWriteEdge.scala:133)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$$anonfun$writeFiles$1.apply(TransactionalWriteEdge.scala:90)\n\tat com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:430)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:18)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:18)\n\tat com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:411)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:18)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:55)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)\n\tat com.databricks.spark.util.UsageLogger$class.recordOperation(UsageLogger.scala:67)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:67)\n\tat com.databricks.spark.util.UsageLogging$class.recordOperation(UsageLogger.scala:342)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:83)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging$class.recordDeltaOperation(DeltaLogging.scala:108)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:83)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$class.writeFiles(TransactionalWriteEdge.scala:90)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:83)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWrite$class.writeFiles(TransactionalWrite.scala:110)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:83)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:111)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1$$anonfun$apply$1.apply(WriteIntoDelta.scala:71)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1$$anonfun$apply$1.apply(WriteIntoDelta.scala:70)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:207)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:70)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:69)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:832)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.run(WriteIntoDelta.scala:69)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:128)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:150)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:191)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:187)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:117)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1$$anonfun$apply$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:217)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:74)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:169)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:307)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:236)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:2)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:56)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:58)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:60)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:62)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:64)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:66)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw.&lt;init&gt;(command-6765358:68)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw.&lt;init&gt;(command-6765358:70)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw.&lt;init&gt;(command-6765358:72)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read.&lt;init&gt;(command-6765358:74)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$.&lt;init&gt;(command-6765358:78)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$.&lt;clinit&gt;(command-6765358)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$eval$.$print(&lt;notebook&gt;:6)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\n\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:215)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:714)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:667)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:396)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:373)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:645)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:640)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 25 in stage 201.0 failed 4 times, most recent failure: Lost task 25.3 in stage 201.0 (TID 4519, 10.0.234.148, executor 1): com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@databricks.com/demo/customer_t2/part-00118-58d2e7ba-a139-44ce-b220-c6c150356875-c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.databricks.com/delta/delta-intro.html#frequently-asked-questions\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:361)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:327)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anonfun$prepareNextFile$1.apply(FileScanRDD.scala:493)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anonfun$prepareNextFile$1.apply(FileScanRDD.scala:481)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply$mcV$sp(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper$class.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t2/part-00118-58d2e7ba-a139-44ce-b220-c6c150356875-c000.snappy.parquet\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1$$anonfun$apply$15.apply(DatabricksFileSystemV2.scala:773)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1$$anonfun$apply$15.apply(DatabricksFileSystemV2.scala:759)\n\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:119)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1.apply(DatabricksFileSystemV2.scala:759)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1.apply(DatabricksFileSystemV2.scala:759)\n\tat com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:430)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:411)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:758)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.getFileStatus(FileSystemWithMetrics.scala:295)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:405)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:128)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:478)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:390)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:311)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2582)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:170)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$$anonfun$writeFiles$1$$anonfun$apply$1.apply(TransactionalWriteEdge.scala:169)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$$anonfun$writeFiles$1$$anonfun$apply$1.apply(TransactionalWriteEdge.scala:133)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1$$anonfun$apply$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:217)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:74)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:169)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$$anonfun$writeFiles$1.apply(TransactionalWriteEdge.scala:133)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$$anonfun$writeFiles$1.apply(TransactionalWriteEdge.scala:90)\n\tat com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:430)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:18)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:18)\n\tat com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:411)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:18)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:55)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)\n\tat com.databricks.spark.util.UsageLogger$class.recordOperation(UsageLogger.scala:67)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:67)\n\tat com.databricks.spark.util.UsageLogging$class.recordOperation(UsageLogger.scala:342)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:83)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging$class.recordDeltaOperation(DeltaLogging.scala:108)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:83)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge$class.writeFiles(TransactionalWriteEdge.scala:90)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:83)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWrite$class.writeFiles(TransactionalWrite.scala:110)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:83)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:111)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1$$anonfun$apply$1.apply(WriteIntoDelta.scala:71)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1$$anonfun$apply$1.apply(WriteIntoDelta.scala:70)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:207)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:70)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:69)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:832)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.run(WriteIntoDelta.scala:69)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:128)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:150)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:191)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:187)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:117)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1$$anonfun$apply$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:217)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:74)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:169)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:307)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:236)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:2)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:56)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:58)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:60)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:62)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:64)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-6765358:66)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw$$iw.&lt;init&gt;(command-6765358:68)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw$$iw.&lt;init&gt;(command-6765358:70)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$$iw.&lt;init&gt;(command-6765358:72)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read.&lt;init&gt;(command-6765358:74)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$.&lt;init&gt;(command-6765358:78)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$read$.&lt;clinit&gt;(command-6765358)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$eval$.$print(&lt;notebook&gt;:6)\n\tat line691cf10a40f74a0f9becc9b8d6cdce0878.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\n\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:215)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:714)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:667)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:396)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:373)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:645)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:640)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@databricks.com/demo/customer_t2/part-00118-58d2e7ba-a139-44ce-b220-c6c150356875-c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.databricks.com/delta/delta-intro.html#frequently-asked-questions\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:361)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:327)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anonfun$prepareNextFile$1.apply(FileScanRDD.scala:493)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anonfun$prepareNextFile$1.apply(FileScanRDD.scala:481)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply$mcV$sp(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper$class.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t2/part-00118-58d2e7ba-a139-44ce-b220-c6c150356875-c000.snappy.parquet\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1$$anonfun$apply$15.apply(DatabricksFileSystemV2.scala:773)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1$$anonfun$apply$15.apply(DatabricksFileSystemV2.scala:759)\n\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:119)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1.apply(DatabricksFileSystemV2.scala:759)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1.apply(DatabricksFileSystemV2.scala:759)\n\tat com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:430)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:411)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:758)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.getFileStatus(FileSystemWithMetrics.scala:295)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:405)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:128)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:478)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:390)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:311)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anonfun$prepareNextFile$1.apply(FileScanRDD.scala:493)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anonfun$prepareNextFile$1.apply(FileScanRDD.scala:481)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply$mcV$sp(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper$class.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#####RECOMMENDATION: Setting Retention policy with cloud provider as well"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"df3d0b7a-69af-442d-a72f-dce7602dca78","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) 5. Other Recommendations"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"316c5e5e-29ca-40c2-afa0-81ba083ad27e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###5A. Improve query performance"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"02ca2428-be0b-4bd1-8ca7-ad4b5c52d57e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"43529a9f-0407-4931-9360-179f57f7f2de","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\nval path = \"dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t6\"\nval dest_path = \"/tmp/vini/customer_comp\"\nval numFiles = 16\n\nspark.read\n .format(\"delta\")\n .load(path)\n .repartition(numFiles)\n .write\n// .option(\"dataChange\", \"false\")\n .format(\"delta\")\n .mode(\"overwrite\")\n .save(\"/tmp/vini/customer_comp\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"c6edf326-b8aa-46bb-b4a8-0cfbe3cb7dfa","inputWidgets":{},"title":"COMPACTION"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">path: String = dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t6\ndest_path: String = /tmp/vini/customer_comp\nnumFiles: Int = 16\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">path: String = dbfs:/Users/vini.jaiswal@databricks.com/demo/customer_t6\ndest_path: String = /tmp/vini/customer_comp\nnumFiles: Int = 16\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#####AUTO OPTIMIZE\n\n\nAuto Optimize consists of two complementary features: Optimized Writes and Auto Compaction.\n1. Optimized Writes\n2. Auto Compaction\n\n--- Available in Databricks runtime version 5.5 or above\n\n#####USAGE:\nTo ensure all new Delta tables have these features enabled, set the SQL configuration:\n\n`````spark.sql(\"set spark.databricks.delta.autoCompact.enabled = true\")`````\n`````spark.sql(\"set spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite = true\")`````"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2c8bea9e-09e1-485a-ae56-96b723b6b8d2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nALTER TABLE delta.`/tmp/vini/customer_comp` SET TBLPROPERTIES (delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"a688deca-c861-4630-87ed-7f224dcbd2b6","inputWidgets":{},"title":"Auto Optimize for existing tables"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nspark.sql(\"select count(1) from delta.`/tmp/vini/customer_comp` where c_customer_id in (select c_customer_id from gdpr.customer_delete_keys)\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"053cdd11-d415-46e3-a5f3-51b636926240","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+\n|count(1)|\n+--------+\n| 6500000|\n+--------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+\ncount(1)|\n+--------+\n 6500000|\n+--------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nspark.sql(\"\"\"DELETE \nFROM   delta.`/tmp/vini/customer_comp` AS t1 \nWHERE  EXISTS \n       ( \n              SELECT c_customer_id \n              FROM   gdpr.customer_delete_keys\n              WHERE  t1.c_customer_id = c_customer_id)\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"50925afe-350b-494a-820d-bfae30c0e8fc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"res28","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">res28: org.apache.spark.sql.DataFrame = []\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res28: org.apache.spark.sql.DataFrame = []\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["###5B. Pseudonymize data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"87c62f00-a322-4708-ba2c-b9bd9417b00b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Your personal data is any information that can be used to directly or indirectly identify you. \n\nIn our pseudonymization scenario, we create a gdpr.customers_lookup  table that contains the real email address and an additional column for a pseudonymized email address.  Now, we can use the pseudo email address in the rest of the data.\n\nWhen there is a request to forget this information, we can simply delete information from the `gdpr.customers_lookup` table and the rest of the information can remain non-identifiable forever.  No further deletes are needed in any other dataset because when the link mapping is deleted, the data may become anonymized already. \n\n<a href=\"https://demo.cloud.databricks.com/#notebook/6758307/command/6763615\">Pseudonymization Notebook</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a7533b31-8136-4960-8d2b-06ddc69de5b6","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###Conclusion\n\nWe covered how you can use Delta best practices to solve the requests from GDPR and CCPA.\n\n- **Z ordering** the data on the keys of delete will help with speeding up identifying and rewriting the impacted files\n- If you delete statement is predicated like colName='xyz' Delta's internal bloom filters will help filter out irrelevant files\n- Ensure the key columns are a part of the first 32 columns in a table, Delta collects stats on the first 32 columns of a table, and these stats help with identifying relevant files for delete operation\n- If you can reduce the size of the source **(BroadcastHashJoin)**, you can leverage dynamic file pruning in determining relevant files for deletes. \n- With use of **Auto Optimize**, Databricks dynamically optimizes Apache Spark partition sizes \n- Use of **Vacuum** operation and **retention policies** to comply with the requests\n- Use of **Pseudonymization techniques** to unidentify any information that can be used to directly or indirectly identify."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a5234e87-7a76-4790-ab87-354004847548","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###References\n\n#####To learn more about Delta Lake on Databricks, see <a href=\"http://docs.databricks.com/delta/index.html\">Delta Lake</a>.\n\n#####For blogs about using Delta Lake for GDPR and CCPA compliance written by Databricks experts, see:\n- How to Avoid Drowning in GDPR Data Subject Requests in a Data Lake\n- Make Your Data Lake CCPA Compliant with a Unified Approach to Data and Analytics\n- Efficient Upserts into Data Lakes with Databricks Delta\n- To learn about purging personal information in the Databricks workspace, see Manage Workspace Storage.\n\n#####Other References\n\n- <a href=\"https://docs.databricks.com/security/privacy/gdpr-delta.html\">Best Practice Guide</a>\n- <a href=\"https://github.com/databricks/tech-talks\">Tech Talk Repo</a>\n- <a href=\"https://www.youtube.com/channel/UC3q8O3Bh2Le8Rj1-Q-_UUbA\">Upcoming Session</a>\n- <a href=\"https://databricks.com/blog/2021/06/22/get-your-free-copy-of-delta-lake-the-definitive-guide-early-release.html\">Delta Lake: The Definitive Guide</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"671095f7-240c-4feb-875d-cbddc5bbcf1c","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Delta_GDPR_CCPA_BestPractices","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":6718320}},"nbformat":4,"nbformat_minor":0}
